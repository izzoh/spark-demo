{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/iris.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the CSV file into a RDD\n",
    "irisData = sc.textFile(\"data/iris.csv\")\n",
    "irisData.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove the first line (contains headers)\n",
    "dataLines = irisData.filter(lambda x: \"Sepal\" not in x)\n",
    "dataLines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transformToNumeric( inputStr) :\n",
    "    attList=inputStr.split(\",\")\n",
    "    \n",
    "    #Set default to setosa\n",
    "    irisValue=1.0\n",
    "    if attList[4] == \"versicolor\":\n",
    "        irisValue=2.0\n",
    "    if attList[4] == \"virginica\":\n",
    "        irisValue=3.0\n",
    "       \n",
    "    #Filter out columns not wanted at this stage\n",
    "    values= Vectors.dense([ irisValue, \\\n",
    "                     float(attList[0]),  \\\n",
    "                     float(attList[1]),  \\\n",
    "                     float(attList[2]),  \\\n",
    "                     float(attList[3])  \\\n",
    "                     ])\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([1.0, 5.1, 3.5, 1.4, 0.2]),\n",
       " DenseVector([1.0, 4.9, 3.0, 1.4, 0.2]),\n",
       " DenseVector([1.0, 4.7, 3.2, 1.3, 0.2]),\n",
       " DenseVector([1.0, 4.6, 3.1, 1.5, 0.2]),\n",
       " DenseVector([1.0, 5.0, 3.6, 1.4, 0.2]),\n",
       " DenseVector([1.0, 5.4, 3.9, 1.7, 0.4]),\n",
       " DenseVector([1.0, 4.6, 3.4, 1.4, 0.3]),\n",
       " DenseVector([1.0, 5.0, 3.4, 1.5, 0.2]),\n",
       " DenseVector([1.0, 4.4, 2.9, 1.4, 0.2]),\n",
       " DenseVector([1.0, 4.9, 3.1, 1.5, 0.1]),\n",
       " DenseVector([1.0, 5.4, 3.7, 1.5, 0.2]),\n",
       " DenseVector([1.0, 4.8, 3.4, 1.6, 0.2]),\n",
       " DenseVector([1.0, 4.8, 3.0, 1.4, 0.1]),\n",
       " DenseVector([1.0, 4.3, 3.0, 1.1, 0.1]),\n",
       " DenseVector([1.0, 5.8, 4.0, 1.2, 0.2]),\n",
       " DenseVector([1.0, 5.7, 4.4, 1.5, 0.4]),\n",
       " DenseVector([1.0, 5.4, 3.9, 1.3, 0.4]),\n",
       " DenseVector([1.0, 5.1, 3.5, 1.4, 0.3]),\n",
       " DenseVector([1.0, 5.7, 3.8, 1.7, 0.3]),\n",
       " DenseVector([1.0, 5.1, 3.8, 1.5, 0.3]),\n",
       " DenseVector([1.0, 5.4, 3.4, 1.7, 0.2]),\n",
       " DenseVector([1.0, 5.1, 3.7, 1.5, 0.4]),\n",
       " DenseVector([1.0, 4.6, 3.6, 1.0, 0.2]),\n",
       " DenseVector([1.0, 5.1, 3.3, 1.7, 0.5]),\n",
       " DenseVector([1.0, 4.8, 3.4, 1.9, 0.2]),\n",
       " DenseVector([1.0, 5.0, 3.0, 1.6, 0.2]),\n",
       " DenseVector([1.0, 5.0, 3.4, 1.6, 0.4]),\n",
       " DenseVector([1.0, 5.2, 3.5, 1.5, 0.2]),\n",
       " DenseVector([1.0, 5.2, 3.4, 1.4, 0.2]),\n",
       " DenseVector([1.0, 4.7, 3.2, 1.6, 0.2]),\n",
       " DenseVector([1.0, 4.8, 3.1, 1.6, 0.2]),\n",
       " DenseVector([1.0, 5.4, 3.4, 1.5, 0.4]),\n",
       " DenseVector([1.0, 5.2, 4.1, 1.5, 0.1]),\n",
       " DenseVector([1.0, 5.5, 4.2, 1.4, 0.2]),\n",
       " DenseVector([1.0, 4.9, 3.1, 1.5, 0.2]),\n",
       " DenseVector([1.0, 5.0, 3.2, 1.2, 0.2]),\n",
       " DenseVector([1.0, 5.5, 3.5, 1.3, 0.2]),\n",
       " DenseVector([1.0, 4.9, 3.6, 1.4, 0.1]),\n",
       " DenseVector([1.0, 4.4, 3.0, 1.3, 0.2]),\n",
       " DenseVector([1.0, 5.1, 3.4, 1.5, 0.2]),\n",
       " DenseVector([1.0, 5.0, 3.5, 1.3, 0.3]),\n",
       " DenseVector([1.0, 4.5, 2.3, 1.3, 0.3]),\n",
       " DenseVector([1.0, 4.4, 3.2, 1.3, 0.2]),\n",
       " DenseVector([1.0, 5.0, 3.5, 1.6, 0.6]),\n",
       " DenseVector([1.0, 5.1, 3.8, 1.9, 0.4]),\n",
       " DenseVector([1.0, 4.8, 3.0, 1.4, 0.3]),\n",
       " DenseVector([1.0, 5.1, 3.8, 1.6, 0.2]),\n",
       " DenseVector([1.0, 4.6, 3.2, 1.4, 0.2]),\n",
       " DenseVector([1.0, 5.3, 3.7, 1.5, 0.2]),\n",
       " DenseVector([1.0, 5.0, 3.3, 1.4, 0.2]),\n",
       " DenseVector([2.0, 7.0, 3.2, 4.7, 1.4]),\n",
       " DenseVector([2.0, 6.4, 3.2, 4.5, 1.5]),\n",
       " DenseVector([2.0, 6.9, 3.1, 4.9, 1.5]),\n",
       " DenseVector([2.0, 5.5, 2.3, 4.0, 1.3]),\n",
       " DenseVector([2.0, 6.5, 2.8, 4.6, 1.5]),\n",
       " DenseVector([2.0, 5.7, 2.8, 4.5, 1.3]),\n",
       " DenseVector([2.0, 6.3, 3.3, 4.7, 1.6]),\n",
       " DenseVector([2.0, 4.9, 2.4, 3.3, 1.0]),\n",
       " DenseVector([2.0, 6.6, 2.9, 4.6, 1.3]),\n",
       " DenseVector([2.0, 5.2, 2.7, 3.9, 1.4]),\n",
       " DenseVector([2.0, 5.0, 2.0, 3.5, 1.0]),\n",
       " DenseVector([2.0, 5.9, 3.0, 4.2, 1.5]),\n",
       " DenseVector([2.0, 6.0, 2.2, 4.0, 1.0]),\n",
       " DenseVector([2.0, 6.1, 2.9, 4.7, 1.4]),\n",
       " DenseVector([2.0, 5.6, 2.9, 3.6, 1.3]),\n",
       " DenseVector([2.0, 6.7, 3.1, 4.4, 1.4]),\n",
       " DenseVector([2.0, 5.6, 3.0, 4.5, 1.5]),\n",
       " DenseVector([2.0, 5.8, 2.7, 4.1, 1.0]),\n",
       " DenseVector([2.0, 6.2, 2.2, 4.5, 1.5]),\n",
       " DenseVector([2.0, 5.6, 2.5, 3.9, 1.1]),\n",
       " DenseVector([2.0, 5.9, 3.2, 4.8, 1.8]),\n",
       " DenseVector([2.0, 6.1, 2.8, 4.0, 1.3]),\n",
       " DenseVector([2.0, 6.3, 2.5, 4.9, 1.5]),\n",
       " DenseVector([2.0, 6.1, 2.8, 4.7, 1.2]),\n",
       " DenseVector([2.0, 6.4, 2.9, 4.3, 1.3]),\n",
       " DenseVector([2.0, 6.6, 3.0, 4.4, 1.4]),\n",
       " DenseVector([2.0, 6.8, 2.8, 4.8, 1.4]),\n",
       " DenseVector([2.0, 6.7, 3.0, 5.0, 1.7]),\n",
       " DenseVector([2.0, 6.0, 2.9, 4.5, 1.5]),\n",
       " DenseVector([2.0, 5.7, 2.6, 3.5, 1.0]),\n",
       " DenseVector([2.0, 5.5, 2.4, 3.8, 1.1]),\n",
       " DenseVector([2.0, 5.5, 2.4, 3.7, 1.0]),\n",
       " DenseVector([2.0, 5.8, 2.7, 3.9, 1.2]),\n",
       " DenseVector([2.0, 6.0, 2.7, 5.1, 1.6]),\n",
       " DenseVector([2.0, 5.4, 3.0, 4.5, 1.5]),\n",
       " DenseVector([2.0, 6.0, 3.4, 4.5, 1.6]),\n",
       " DenseVector([2.0, 6.7, 3.1, 4.7, 1.5]),\n",
       " DenseVector([2.0, 6.3, 2.3, 4.4, 1.3]),\n",
       " DenseVector([2.0, 5.6, 3.0, 4.1, 1.3]),\n",
       " DenseVector([2.0, 5.5, 2.5, 4.0, 1.3]),\n",
       " DenseVector([2.0, 5.5, 2.6, 4.4, 1.2]),\n",
       " DenseVector([2.0, 6.1, 3.0, 4.6, 1.4]),\n",
       " DenseVector([2.0, 5.8, 2.6, 4.0, 1.2]),\n",
       " DenseVector([2.0, 5.0, 2.3, 3.3, 1.0]),\n",
       " DenseVector([2.0, 5.6, 2.7, 4.2, 1.3]),\n",
       " DenseVector([2.0, 5.7, 3.0, 4.2, 1.2]),\n",
       " DenseVector([2.0, 5.7, 2.9, 4.2, 1.3]),\n",
       " DenseVector([2.0, 6.2, 2.9, 4.3, 1.3]),\n",
       " DenseVector([2.0, 5.1, 2.5, 3.0, 1.1]),\n",
       " DenseVector([2.0, 5.7, 2.8, 4.1, 1.3]),\n",
       " DenseVector([3.0, 6.3, 3.3, 6.0, 2.5]),\n",
       " DenseVector([3.0, 5.8, 2.7, 5.1, 1.9]),\n",
       " DenseVector([3.0, 7.1, 3.0, 5.9, 2.1]),\n",
       " DenseVector([3.0, 6.3, 2.9, 5.6, 1.8]),\n",
       " DenseVector([3.0, 6.5, 3.0, 5.8, 2.2]),\n",
       " DenseVector([3.0, 7.6, 3.0, 6.6, 2.1]),\n",
       " DenseVector([3.0, 4.9, 2.5, 4.5, 1.7]),\n",
       " DenseVector([3.0, 7.3, 2.9, 6.3, 1.8]),\n",
       " DenseVector([3.0, 6.7, 2.5, 5.8, 1.8]),\n",
       " DenseVector([3.0, 7.2, 3.6, 6.1, 2.5]),\n",
       " DenseVector([3.0, 6.5, 3.2, 5.1, 2.0]),\n",
       " DenseVector([3.0, 6.4, 2.7, 5.3, 1.9]),\n",
       " DenseVector([3.0, 6.8, 3.0, 5.5, 2.1]),\n",
       " DenseVector([3.0, 5.7, 2.5, 5.0, 2.0]),\n",
       " DenseVector([3.0, 5.8, 2.8, 5.1, 2.4]),\n",
       " DenseVector([3.0, 6.4, 3.2, 5.3, 2.3]),\n",
       " DenseVector([3.0, 6.5, 3.0, 5.5, 1.8]),\n",
       " DenseVector([3.0, 7.7, 3.8, 6.7, 2.2]),\n",
       " DenseVector([3.0, 7.7, 2.6, 6.9, 2.3]),\n",
       " DenseVector([3.0, 6.0, 2.2, 5.0, 1.5]),\n",
       " DenseVector([3.0, 6.9, 3.2, 5.7, 2.3]),\n",
       " DenseVector([3.0, 5.6, 2.8, 4.9, 2.0]),\n",
       " DenseVector([3.0, 7.7, 2.8, 6.7, 2.0]),\n",
       " DenseVector([3.0, 6.3, 2.7, 4.9, 1.8]),\n",
       " DenseVector([3.0, 6.7, 3.3, 5.7, 2.1]),\n",
       " DenseVector([3.0, 7.2, 3.2, 6.0, 1.8]),\n",
       " DenseVector([3.0, 6.2, 2.8, 4.8, 1.8]),\n",
       " DenseVector([3.0, 6.1, 3.0, 4.9, 1.8]),\n",
       " DenseVector([3.0, 6.4, 2.8, 5.6, 2.1]),\n",
       " DenseVector([3.0, 7.2, 3.0, 5.8, 1.6]),\n",
       " DenseVector([3.0, 7.4, 2.8, 6.1, 1.9]),\n",
       " DenseVector([3.0, 7.9, 3.8, 6.4, 2.0]),\n",
       " DenseVector([3.0, 6.4, 2.8, 5.6, 2.2]),\n",
       " DenseVector([3.0, 6.3, 2.8, 5.1, 1.5]),\n",
       " DenseVector([3.0, 6.1, 2.6, 5.6, 1.4]),\n",
       " DenseVector([3.0, 7.7, 3.0, 6.1, 2.3]),\n",
       " DenseVector([3.0, 6.3, 3.4, 5.6, 2.4]),\n",
       " DenseVector([3.0, 6.4, 3.1, 5.5, 1.8]),\n",
       " DenseVector([3.0, 6.0, 3.0, 4.8, 1.8]),\n",
       " DenseVector([3.0, 6.9, 3.1, 5.4, 2.1]),\n",
       " DenseVector([3.0, 6.7, 3.1, 5.6, 2.4]),\n",
       " DenseVector([3.0, 6.9, 3.1, 5.1, 2.3]),\n",
       " DenseVector([3.0, 5.8, 2.7, 5.1, 1.9]),\n",
       " DenseVector([3.0, 6.8, 3.2, 5.9, 2.3]),\n",
       " DenseVector([3.0, 6.7, 3.3, 5.7, 2.5]),\n",
       " DenseVector([3.0, 6.7, 3.0, 5.2, 2.3]),\n",
       " DenseVector([3.0, 6.3, 2.5, 5.0, 1.9]),\n",
       " DenseVector([3.0, 6.5, 3.0, 5.2, 2.0]),\n",
       " DenseVector([3.0, 6.2, 3.4, 5.4, 2.3]),\n",
       " DenseVector([3.0, 5.9, 3.0, 5.1, 1.8])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Change to a Vector\n",
    "irisVectors = dataLines.map(transformToNumeric)\n",
    "irisVectors.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.        ,  5.84333333,  3.05733333,  3.758     ,  1.19933333])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Perform statistical Analysis\n",
    "from pyspark.mllib.stat import Statistics\n",
    "irisStats=Statistics.colStats(irisVectors)\n",
    "irisStats.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.67114094,  0.68569351,  0.18997942,  3.11627785,  0.58100626])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisStats.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1. ,  4.3,  2. ,  1. ,  0.1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisStats.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3. ,  7.9,  4.4,  6.9,  2.5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisStats.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.78256123, -0.42665756,  0.9490347 ,  0.95654733],\n",
       "       [ 0.78256123,  1.        , -0.11756978,  0.87175378,  0.81794113],\n",
       "       [-0.42665756, -0.11756978,  1.        , -0.4284401 , -0.36612593],\n",
       "       [ 0.9490347 ,  0.87175378, -0.4284401 ,  1.        ,  0.96286543],\n",
       "       [ 0.95654733,  0.81794113, -0.36612593,  0.96286543,  1.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(irisVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Transform to a Data Frame for input to Machine Learing\n",
    "#Drop columns that are not required (low correlation)\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transformToLabeledPoint(inStr) :\n",
    "    attList=inStr.split(\",\")\n",
    "    lp = ( attList[4], Vectors.dense([attList[0],attList[2],attList[3]]))\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "| label|     features|\n",
      "+------+-------------+\n",
      "|setosa|[5.1,1.4,0.2]|\n",
      "|setosa|[4.9,1.4,0.2]|\n",
      "|setosa|[4.7,1.3,0.2]|\n",
      "|setosa|[4.6,1.5,0.2]|\n",
      "|setosa|[5.0,1.4,0.2]|\n",
      "|setosa|[5.4,1.7,0.4]|\n",
      "|setosa|[4.6,1.4,0.3]|\n",
      "|setosa|[5.0,1.5,0.2]|\n",
      "|setosa|[4.4,1.4,0.2]|\n",
      "|setosa|[4.9,1.5,0.1]|\n",
      "+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "irisLp = dataLines.map(transformToLabeledPoint)\n",
    "irisDF = sqlContext.createDataFrame(irisLp,[\"label\", \"features\"])\n",
    "irisDF.select(\"label\",\"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o90.corr.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 14, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\nValueError: could not convert string to float: setosa\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1328)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1302)\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1342)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1341)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:61)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:328)\n\tat org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationMatrix(PearsonCorrelation.scala:49)\n\tat org.apache.spark.mllib.stat.correlation.Correlation$class.computeCorrelationWithMatrixImpl(Correlation.scala:46)\n\tat org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationWithMatrixImpl(PearsonCorrelation.scala:34)\n\tat org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelation(PearsonCorrelation.scala:40)\n\tat org.apache.spark.mllib.stat.correlation.Correlations$.corr(Correlation.scala:60)\n\tat org.apache.spark.mllib.stat.Statistics$.corr(Statistics.scala:112)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.corr(PythonMLLibAPI.scala:820)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\nValueError: could not convert string to float: setosa\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5969aaa86d38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfeatureRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mirisDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelRDD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureRDD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pearson'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%d\\t%g'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/pyspark/mllib/stat/_statistics.pyc\u001b[0m in \u001b[0;36mcorr\u001b[0;34m(x, y, method)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"corr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_to_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"corr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/pyspark/mllib/common.pyc\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/pyspark/mllib/common.pyc\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o90.corr.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 14, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\nValueError: could not convert string to float: setosa\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1328)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1302)\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1342)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1341)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:61)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:328)\n\tat org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationMatrix(PearsonCorrelation.scala:49)\n\tat org.apache.spark.mllib.stat.correlation.Correlation$class.computeCorrelationWithMatrixImpl(Correlation.scala:46)\n\tat org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationWithMatrixImpl(PearsonCorrelation.scala:34)\n\tat org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelation(PearsonCorrelation.scala:40)\n\tat org.apache.spark.mllib.stat.correlation.Correlations$.corr(Correlation.scala:60)\n\tat org.apache.spark.mllib.stat.Statistics$.corr(Statistics.scala:112)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.corr(PythonMLLibAPI.scala:820)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/isaac/spark/spark-1.6.3-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\nValueError: could not convert string to float: setosa\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#Find correlations\n",
    "numFeatures = irisDF.take(1)[0].features.size\n",
    "labelRDD = irisDF.map(lambda lp: lp.label)\n",
    "for i in range(numFeatures):\n",
    "    featureRDD = irisDF.map(lambda lp: lp.features[i])\n",
    "    corr = Statistics.corr(labelRDD, featureRDD, 'pearson')\n",
    "    print('%d\\t%g' % (i, corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Indexing needed as pre-req for Decision Trees\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\n",
    "si_model = stringIndexer.fit(irisDF)\n",
    "td = si_model.transform(irisDF)\n",
    "td.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Split into training and testing data\n",
    "(trainingData, testData) = td.randomSplit([0.9, 0.1])\n",
    "trainingData.count()\n",
    "testData.count()\n",
    "testData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create the model\n",
    "dtClassifer = DecisionTreeClassifier(maxDepth=2, labelCol=\"indexed\")\n",
    "dtModel = dtClassifer.fit(trainingData)\n",
    "\n",
    "dtModel.numNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dtModel.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Predict on the test data\n",
    "predictions = dtModel.transform(trainingData)\n",
    "predictions.select(\"prediction\",\"indexed\",\"label\",\"features\").collect()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \\\n",
    "                    labelCol=\"indexed\",metricName=\"precision\")\n",
    "evaluator.evaluate(predictions)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Draw a confusion matrix\n",
    "labelList=predictions.select(\"indexed\",\"label\").distinct().toPandas()\n",
    "predictions.groupBy(\"indexed\",\"prediction\").count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
