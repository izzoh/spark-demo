{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the CSV file into a RDD\n",
    "bankData = sc.textFile(\"data/bank.csv\")\n",
    "bankData.cache()\n",
    "bankData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove the first line (contains headers)\n",
    "firstLine=bankData.first()\n",
    "dataLines = bankData.filter(lambda x: x != firstLine)\n",
    "dataLines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "def transformToNumeric( inputStr) :\n",
    "    \n",
    "    attList=inputStr.replace(\"\\\"\",\"\").split(\";\")\n",
    "    \n",
    "    age=float(attList[0])\n",
    "    #convert outcome to float    \n",
    "    outcome = 0.0 if attList[16] == \"no\" else 1.0\n",
    "    \n",
    "    #create indicator variables for single/married    \n",
    "    single= 1.0 if attList[2] == \"single\" else 0.0\n",
    "    married = 1.0 if attList[2] == \"married\" else 0.0\n",
    "    divorced = 1.0 if attList[2] == \"divorced\" else 0.0\n",
    "    \n",
    "    #create indicator variables for education\n",
    "    primary = 1.0 if attList[3] == \"primary\" else 0.0\n",
    "    secondary = 1.0 if attList[3] == \"secondary\" else 0.0\n",
    "    tertiary = 1.0 if attList[3] == \"tertiary\" else 0.0\n",
    "    \n",
    "    #convert default to float\n",
    "    default= 0.0 if attList[4] == \"no\" else 1.0\n",
    "    #convert balance amount to float\n",
    "    balance=float(attList[5])\n",
    "    #convert loan to float\n",
    "    loan= 0.0 if attList[7] == \"no\" else 1.0\n",
    "    \n",
    "    #Filter out columns not wanted at this stage\n",
    "    values= Vectors.dense([ outcome, age, single, married, \\\n",
    "                divorced, primary, secondary, tertiary,\\\n",
    "                default, balance, loan \\\n",
    "                     ])\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Change to a Vector\n",
    "bankVectors = dataLines.map(transformToNumeric)\n",
    "bankVectors.collect()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perform statistical Analysis\n",
    "from pyspark.mllib.stat import Statistics\n",
    "bankStats=Statistics.colStats(bankVectors)\n",
    "bankStats.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bankStats.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bankStats.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bankStats.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Statistics.corr(bankVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transform to a Data Frame for input to Machine Learing\n",
    "#Drop columns that are not required (low correlation)\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformToLabeledPoint(inStr) :\n",
    "    lp = ( float(inStr[0]), \\\n",
    "    Vectors.dense([inStr[1],inStr[2],inStr[3], \\\n",
    "        inStr[4],inStr[5],inStr[6],inStr[7], \\\n",
    "        inStr[8],inStr[9],inStr[10]\n",
    "        ]))\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bankLp = bankVectors.map(transformToLabeledPoint)\n",
    "bankLp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bankDF = sqlContext.createDataFrame(bankLp,[\"label\", \"features\"])\n",
    "bankDF.select(\"label\",\"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perform PCA\n",
    "from pyspark.ml.feature import PCA\n",
    "bankPCA = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "pcaModel = bankPCA.fit(bankDF)\n",
    "pcaResult = pcaModel.transform(bankDF).select(\"label\",\"pcaFeatures\")\n",
    "pcaResult.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Indexing needed as pre-req for Decision Trees\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\n",
    "si_model = stringIndexer.fit(pcaResult)\n",
    "td = si_model.transform(pcaResult)\n",
    "td.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split into training and testing data\n",
    "(trainingData, testData) = td.randomSplit([0.7, 0.3])\n",
    "trainingData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the model\n",
    "rmClassifer = RandomForestClassifier(labelCol=\"indexed\", \\\n",
    "                featuresCol=\"pcaFeatures\")\n",
    "rmModel = rmClassifer.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict on the test data\n",
    "predictions = rmModel.transform(testData)\n",
    "predictions.select(\"prediction\",\"indexed\",\"label\",\"pcaFeatures\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \\\n",
    "                    labelCol=\"indexed\",metricName=\"precision\")\n",
    "evaluator.evaluate(predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Draw a confusion matrix\n",
    "labelList=predictions.select(\"indexed\",\"label\").distinct().toPandas()\n",
    "predictions.groupBy(\"indexed\",\"prediction\").count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
