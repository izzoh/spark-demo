{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/auto-data.csv MapPartitionsRDD[25] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the CSV file into a RDD\n",
    "autoData = sc.textFile(\"data/auto-data.csv\")\n",
    "autoData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove the first line (contains headers)\n",
    "firstLine = autoData.first()\n",
    "dataLines = autoData.filter(lambda x: x != firstLine)\n",
    "dataLines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext,Row\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Convert to Local Vector.\n",
    "def transformToNumeric( inputStr) :\n",
    "    attList=inputStr.split(\",\")\n",
    "\n",
    "    doors = 1.0 if attList[3] ==\"two\" else 2.0\n",
    "    body = 1.0 if attList[4] == \"sedan\" else 2.0 \n",
    "       \n",
    "    #Filter out columns not wanted at this stage\n",
    "    values= Vectors.dense([ doors, \\\n",
    "                     float(body),  \\\n",
    "                     float(attList[7]),  \\\n",
    "                     float(attList[8]),  \\\n",
    "                     float(attList[9])  \\\n",
    "                     ])\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[29] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoVector = dataLines.map(transformToNumeric)\n",
    "autoVector.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([1.0, 2.0, 69.0, 4900.0, 31.0]),\n",
       " DenseVector([1.0, 2.0, 48.0, 5100.0, 47.0]),\n",
       " DenseVector([1.0, 2.0, 68.0, 5000.0, 30.0]),\n",
       " DenseVector([1.0, 2.0, 62.0, 4800.0, 35.0]),\n",
       " DenseVector([1.0, 2.0, 68.0, 5500.0, 37.0]),\n",
       " DenseVector([1.0, 2.0, 60.0, 5500.0, 38.0]),\n",
       " DenseVector([1.0, 1.0, 69.0, 5200.0, 31.0]),\n",
       " DenseVector([1.0, 2.0, 68.0, 5500.0, 37.0]),\n",
       " DenseVector([1.0, 2.0, 68.0, 5500.0, 37.0]),\n",
       " DenseVector([1.0, 2.0, 68.0, 5000.0, 31.0]),\n",
       " DenseVector([1.0, 2.0, 68.0, 5500.0, 31.0]),\n",
       " DenseVector([2.0, 2.0, 68.0, 5500.0, 31.0]),\n",
       " DenseVector([2.0, 2.0, 68.0, 5500.0, 31.0]),\n",
       " DenseVector([1.0, 2.0, 70.0, 5400.0, 38.0]),\n",
       " DenseVector([1.0, 2.0, 62.0, 4800.0, 31.0]),\n",
       " DenseVector([1.0, 2.0, 68.0, 5500.0, 31.0]),\n",
       " DenseVector([1.0, 2.0, 58.0, 4800.0, 49.0]),\n",
       " DenseVector([2.0, 2.0, 62.0, 4800.0, 31.0]),\n",
       " DenseVector([1.0, 2.0, 76.0, 6000.0, 30.0]),\n",
       " DenseVector([2.0, 1.0, 70.0, 5400.0, 38.0]),\n",
       " DenseVector([1.0, 1.0, 69.0, 5200.0, 31.0]),\n",
       " DenseVector([1.0, 2.0, 68.0, 5500.0, 31.0]),\n",
       " DenseVector([2.0, 1.0, 68.0, 5500.0, 31.0]),\n",
       " DenseVector([2.0, 1.0, 68.0, 5500.0, 31.0]),\n",
       " DenseVector([2.0, 1.0, 68.0, 5000.0, 31.0]),\n",
       " DenseVector([2.0, 1.0, 78.0, 4800.0, 24.0]),\n",
       " DenseVector([1.0, 2.0, 68.0, 5000.0, 31.0]),\n",
       " DenseVector([2.0, 1.0, 69.0, 5200.0, 31.0]),\n",
       " DenseVector([1.0, 2.0, 76.0, 6000.0, 31.0]),\n",
       " DenseVector([2.0, 2.0, 62.0, 4800.0, 31.0]),\n",
       " DenseVector([2.0, 1.0, 70.0, 4800.0, 30.0]),\n",
       " DenseVector([2.0, 1.0, 88.0, 5000.0, 25.0]),\n",
       " DenseVector([1.0, 2.0, 73.0, 4400.0, 26.0]),\n",
       " DenseVector([1.0, 1.0, 55.0, 4800.0, 45.0]),\n",
       " DenseVector([2.0, 1.0, 82.0, 4800.0, 32.0]),\n",
       " DenseVector([1.0, 2.0, 76.0, 6000.0, 30.0]),\n",
       " DenseVector([2.0, 2.0, 70.0, 4800.0, 30.0]),\n",
       " DenseVector([2.0, 1.0, 76.0, 6000.0, 30.0]),\n",
       " DenseVector([2.0, 2.0, 76.0, 6000.0, 30.0]),\n",
       " DenseVector([1.0, 1.0, 69.0, 5200.0, 31.0]),\n",
       " DenseVector([2.0, 2.0, 69.0, 5200.0, 31.0]),\n",
       " DenseVector([2.0, 1.0, 68.0, 5000.0, 31.0]),\n",
       " DenseVector([2.0, 2.0, 82.0, 4800.0, 28.0]),\n",
       " DenseVector([2.0, 1.0, 69.0, 5200.0, 31.0]),\n",
       " DenseVector([1.0, 2.0, 73.0, 4400.0, 26.0]),\n",
       " DenseVector([2.0, 1.0, 68.0, 5500.0, 31.0]),\n",
       " DenseVector([2.0, 1.0, 68.0, 5500.0, 31.0]),\n",
       " DenseVector([1.0, 2.0, 102.0, 5500.0, 24.0]),\n",
       " DenseVector([2.0, 1.0, 70.0, 4800.0, 38.0]),\n",
       " DenseVector([2.0, 1.0, 82.0, 4400.0, 28.0]),\n",
       " DenseVector([1.0, 1.0, 52.0, 4800.0, 37.0]),\n",
       " DenseVector([2.0, 2.0, 56.0, 4500.0, 38.0]),\n",
       " DenseVector([1.0, 2.0, 69.0, 5200.0, 31.0]),\n",
       " DenseVector([1.0, 2.0, 86.0, 5800.0, 27.0]),\n",
       " DenseVector([2.0, 2.0, 62.0, 4800.0, 27.0]),\n",
       " DenseVector([2.0, 1.0, 56.0, 4500.0, 34.0]),\n",
       " DenseVector([1.0, 2.0, 102.0, 5500.0, 24.0]),\n",
       " DenseVector([1.0, 2.0, 102.0, 5500.0, 24.0]),\n",
       " DenseVector([1.0, 1.0, 85.0, 5250.0, 27.0]),\n",
       " DenseVector([2.0, 1.0, 52.0, 4800.0, 37.0]),\n",
       " DenseVector([2.0, 2.0, 69.0, 5200.0, 31.0]),\n",
       " DenseVector([2.0, 2.0, 82.0, 4800.0, 23.0]),\n",
       " DenseVector([1.0, 1.0, 70.0, 4800.0, 29.0]),\n",
       " DenseVector([2.0, 1.0, 88.0, 5000.0, 25.0]),\n",
       " DenseVector([2.0, 1.0, 85.0, 5250.0, 27.0]),\n",
       " DenseVector([1.0, 2.0, 70.0, 4800.0, 29.0]),\n",
       " DenseVector([1.0, 2.0, 69.0, 5200.0, 31.0]),\n",
       " DenseVector([2.0, 2.0, 70.0, 4800.0, 28.0]),\n",
       " DenseVector([1.0, 2.0, 116.0, 4800.0, 24.0]),\n",
       " DenseVector([2.0, 1.0, 84.0, 4800.0, 26.0]),\n",
       " DenseVector([2.0, 1.0, 85.0, 5250.0, 27.0]),\n",
       " DenseVector([1.0, 2.0, 88.0, 5000.0, 25.0]),\n",
       " DenseVector([2.0, 2.0, 62.0, 4800.0, 27.0]),\n",
       " DenseVector([2.0, 1.0, 86.0, 5800.0, 27.0]),\n",
       " DenseVector([1.0, 2.0, 84.0, 4800.0, 26.0]),\n",
       " DenseVector([2.0, 2.0, 88.0, 5000.0, 24.0]),\n",
       " DenseVector([2.0, 2.0, 88.0, 5000.0, 24.0]),\n",
       " DenseVector([2.0, 1.0, 92.0, 4200.0, 29.0]),\n",
       " DenseVector([2.0, 2.0, 97.0, 5200.0, 27.0]),\n",
       " DenseVector([1.0, 2.0, 86.0, 5800.0, 27.0]),\n",
       " DenseVector([2.0, 1.0, 82.0, 4800.0, 24.0]),\n",
       " DenseVector([2.0, 1.0, 70.0, 4800.0, 28.0]),\n",
       " DenseVector([2.0, 1.0, 116.0, 5500.0, 23.0]),\n",
       " DenseVector([2.0, 1.0, 116.0, 5500.0, 23.0]),\n",
       " DenseVector([1.0, 1.0, 112.0, 6600.0, 26.0]),\n",
       " DenseVector([2.0, 1.0, 68.0, 4500.0, 37.0]),\n",
       " DenseVector([1.0, 2.0, 112.0, 6600.0, 26.0]),\n",
       " DenseVector([2.0, 1.0, 97.0, 5200.0, 27.0]),\n",
       " DenseVector([1.0, 2.0, 116.0, 4800.0, 24.0]),\n",
       " DenseVector([1.0, 2.0, 116.0, 5500.0, 23.0]),\n",
       " DenseVector([2.0, 1.0, 94.0, 5200.0, 26.0]),\n",
       " DenseVector([1.0, 2.0, 90.0, 5500.0, 24.0]),\n",
       " DenseVector([2.0, 2.0, 92.0, 4200.0, 27.0]),\n",
       " DenseVector([1.0, 2.0, 116.0, 4800.0, 24.0]),\n",
       " DenseVector([2.0, 1.0, 100.0, 5500.0, 26.0]),\n",
       " DenseVector([2.0, 2.0, 94.0, 5200.0, 25.0]),\n",
       " DenseVector([2.0, 1.0, 84.0, 4800.0, 26.0]),\n",
       " DenseVector([2.0, 1.0, 86.0, 5800.0, 27.0]),\n",
       " DenseVector([1.0, 1.0, 100.0, 5500.0, 25.0]),\n",
       " DenseVector([1.0, 2.0, 84.0, 4800.0, 26.0]),\n",
       " DenseVector([2.0, 1.0, 73.0, 4500.0, 30.0]),\n",
       " DenseVector([2.0, 1.0, 92.0, 4200.0, 27.0]),\n",
       " DenseVector([1.0, 2.0, 101.0, 6000.0, 17.0]),\n",
       " DenseVector([1.0, 2.0, 90.0, 5000.0, 24.0]),\n",
       " DenseVector([1.0, 2.0, 116.0, 4800.0, 24.0]),\n",
       " DenseVector([2.0, 2.0, 84.0, 4800.0, 26.0]),\n",
       " DenseVector([2.0, 2.0, 92.0, 4200.0, 27.0]),\n",
       " DenseVector([2.0, 1.0, 111.0, 4800.0, 24.0]),\n",
       " DenseVector([1.0, 2.0, 116.0, 4800.0, 24.0]),\n",
       " DenseVector([1.0, 2.0, 90.0, 5500.0, 24.0]),\n",
       " DenseVector([2.0, 2.0, 111.0, 4800.0, 23.0]),\n",
       " DenseVector([1.0, 2.0, 101.0, 6000.0, 17.0]),\n",
       " DenseVector([1.0, 2.0, 110.0, 5250.0, 21.0]),\n",
       " DenseVector([2.0, 1.0, 97.0, 5000.0, 19.0]),\n",
       " DenseVector([2.0, 1.0, 110.0, 5250.0, 21.0]),\n",
       " DenseVector([2.0, 2.0, 88.0, 5500.0, 25.0]),\n",
       " DenseVector([2.0, 2.0, 97.0, 5000.0, 19.0]),\n",
       " DenseVector([1.0, 2.0, 145.0, 5000.0, 19.0]),\n",
       " DenseVector([1.0, 2.0, 145.0, 5000.0, 19.0]),\n",
       " DenseVector([2.0, 1.0, 114.0, 5400.0, 23.0]),\n",
       " DenseVector([2.0, 1.0, 101.0, 5800.0, 24.0]),\n",
       " DenseVector([1.0, 2.0, 145.0, 5000.0, 19.0]),\n",
       " DenseVector([2.0, 1.0, 95.0, 4150.0, 28.0]),\n",
       " DenseVector([2.0, 1.0, 110.0, 5500.0, 19.0]),\n",
       " DenseVector([2.0, 2.0, 114.0, 5400.0, 23.0]),\n",
       " DenseVector([1.0, 2.0, 111.0, 5000.0, 21.0]),\n",
       " DenseVector([2.0, 1.0, 152.0, 5200.0, 17.0]),\n",
       " DenseVector([2.0, 1.0, 152.0, 5200.0, 19.0]),\n",
       " DenseVector([1.0, 2.0, 101.0, 6000.0, 17.0]),\n",
       " DenseVector([2.0, 1.0, 68.0, 4500.0, 33.0]),\n",
       " DenseVector([2.0, 2.0, 95.0, 4150.0, 25.0]),\n",
       " DenseVector([2.0, 1.0, 102.0, 5500.0, 24.0]),\n",
       " DenseVector([2.0, 2.0, 152.0, 5200.0, 17.0]),\n",
       " DenseVector([1.0, 2.0, 145.0, 5000.0, 19.0]),\n",
       " DenseVector([1.0, 2.0, 145.0, 5000.0, 19.0]),\n",
       " DenseVector([1.0, 2.0, 110.0, 5250.0, 21.0]),\n",
       " DenseVector([1.0, 1.0, 110.0, 5500.0, 19.0]),\n",
       " DenseVector([2.0, 1.0, 110.0, 5250.0, 21.0]),\n",
       " DenseVector([2.0, 1.0, 95.0, 5000.0, 19.0]),\n",
       " DenseVector([1.0, 2.0, 135.0, 6000.0, 16.0]),\n",
       " DenseVector([2.0, 1.0, 156.0, 5200.0, 20.0]),\n",
       " DenseVector([2.0, 2.0, 156.0, 5200.0, 19.0]),\n",
       " DenseVector([2.0, 1.0, 114.0, 5400.0, 24.0]),\n",
       " DenseVector([1.0, 2.0, 161.0, 5200.0, 19.0]),\n",
       " DenseVector([1.0, 1.0, 101.0, 5800.0, 23.0]),\n",
       " DenseVector([1.0, 2.0, 111.0, 5000.0, 21.0]),\n",
       " DenseVector([1.0, 2.0, 154.0, 5000.0, 19.0]),\n",
       " DenseVector([1.0, 2.0, 175.0, 5000.0, 19.0]),\n",
       " DenseVector([2.0, 2.0, 114.0, 5400.0, 24.0]),\n",
       " DenseVector([1.0, 2.0, 161.0, 5200.0, 20.0]),\n",
       " DenseVector([2.0, 1.0, 97.0, 5000.0, 19.0]),\n",
       " DenseVector([2.0, 2.0, 95.0, 5000.0, 19.0]),\n",
       " DenseVector([2.0, 1.0, 114.0, 5400.0, 23.0]),\n",
       " DenseVector([2.0, 1.0, 95.0, 4150.0, 28.0]),\n",
       " DenseVector([2.0, 1.0, 101.0, 5800.0, 23.0]),\n",
       " DenseVector([2.0, 2.0, 95.0, 4150.0, 25.0]),\n",
       " DenseVector([1.0, 2.0, 160.0, 5200.0, 19.0]),\n",
       " DenseVector([2.0, 1.0, 115.0, 5500.0, 18.0]),\n",
       " DenseVector([1.0, 2.0, 116.0, 4800.0, 24.0]),\n",
       " DenseVector([2.0, 1.0, 110.0, 5500.0, 19.0]),\n",
       " DenseVector([2.0, 1.0, 95.0, 4150.0, 28.0]),\n",
       " DenseVector([2.0, 1.0, 142.0, 5600.0, 18.0]),\n",
       " DenseVector([1.0, 2.0, 160.0, 5500.0, 19.0]),\n",
       " DenseVector([2.0, 1.0, 120.0, 5000.0, 19.0]),\n",
       " DenseVector([2.0, 1.0, 72.0, 4200.0, 31.0]),\n",
       " DenseVector([1.0, 2.0, 160.0, 5200.0, 19.0]),\n",
       " DenseVector([2.0, 1.0, 162.0, 5100.0, 17.0]),\n",
       " DenseVector([2.0, 1.0, 160.0, 5500.0, 19.0]),\n",
       " DenseVector([2.0, 2.0, 110.0, 5500.0, 19.0]),\n",
       " DenseVector([2.0, 2.0, 162.0, 5100.0, 17.0]),\n",
       " DenseVector([2.0, 1.0, 160.0, 5300.0, 19.0]),\n",
       " DenseVector([1.0, 2.0, 200.0, 5200.0, 17.0]),\n",
       " DenseVector([1.0, 1.0, 121.0, 4250.0, 21.0]),\n",
       " DenseVector([2.0, 1.0, 121.0, 4250.0, 21.0]),\n",
       " DenseVector([2.0, 1.0, 134.0, 5500.0, 18.0]),\n",
       " DenseVector([1.0, 2.0, 143.0, 5500.0, 19.0]),\n",
       " DenseVector([2.0, 1.0, 106.0, 4800.0, 26.0]),\n",
       " DenseVector([2.0, 1.0, 114.0, 5400.0, 19.0]),\n",
       " DenseVector([2.0, 1.0, 140.0, 5500.0, 17.0]),\n",
       " DenseVector([2.0, 1.0, 121.0, 4250.0, 20.0]),\n",
       " DenseVector([2.0, 1.0, 123.0, 4350.0, 22.0]),\n",
       " DenseVector([1.0, 2.0, 123.0, 4350.0, 22.0]),\n",
       " DenseVector([2.0, 2.0, 123.0, 4350.0, 22.0]),\n",
       " DenseVector([2.0, 1.0, 182.0, 5400.0, 16.0]),\n",
       " DenseVector([2.0, 1.0, 123.0, 4350.0, 22.0]),\n",
       " DenseVector([2.0, 1.0, 176.0, 4750.0, 15.0]),\n",
       " DenseVector([1.0, 2.0, 207.0, 5900.0, 17.0]),\n",
       " DenseVector([1.0, 2.0, 207.0, 5900.0, 17.0]),\n",
       " DenseVector([2.0, 1.0, 155.0, 4750.0, 16.0]),\n",
       " DenseVector([1.0, 2.0, 155.0, 4750.0, 16.0]),\n",
       " DenseVector([2.0, 1.0, 176.0, 4750.0, 15.0]),\n",
       " DenseVector([1.0, 1.0, 262.0, 5000.0, 13.0]),\n",
       " DenseVector([2.0, 1.0, 182.0, 5400.0, 15.0]),\n",
       " DenseVector([1.0, 2.0, 207.0, 5900.0, 17.0]),\n",
       " DenseVector([2.0, 1.0, 184.0, 4500.0, 14.0]),\n",
       " DenseVector([1.0, 1.0, 182.0, 5400.0, 16.0]),\n",
       " DenseVector([1.0, 2.0, 184.0, 4500.0, 14.0])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoVector.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Centering and scaling. To perform this every value should be subtracted\n",
    "#from that column's mean and divided by its Std. Deviation.\n",
    "\n",
    "#Perform statistical Analysis and compute mean and Std.Dev for every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o289.colStats.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 15.0 failed 1 times, most recent failure: Lost task 1.0 in stage 15.0 (TID 26, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 80, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1953)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:984)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1127)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1104)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeColumnSummaryStatistics(RowMatrix.scala:416)\n\tat org.apache.spark.mllib.stat.Statistics$.colStats(Statistics.scala:46)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.colStats(PythonMLLibAPI.scala:821)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 80, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-d8f7a27f56c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStatistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mautoStats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoVector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcolMeans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoStats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcolVariance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoStats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcolStdDev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolVariance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\u001b[0m in \u001b[0;36mcolStats\u001b[0;34m(rdd)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mcStats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"colStats\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_to_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mMultivariateStatisticalSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcStats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o289.colStats.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 15.0 failed 1 times, most recent failure: Lost task 1.0 in stage 15.0 (TID 26, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 80, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1953)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:984)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1127)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1104)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeColumnSummaryStatistics(RowMatrix.scala:416)\n\tat org.apache.spark.mllib.stat.Statistics$.colStats(Statistics.scala:46)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.colStats(PythonMLLibAPI.scala:821)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/isaac/spark/spark-2.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 80, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "autoStats=Statistics.colStats(autoVector)\n",
    "colMeans=autoStats.mean()\n",
    "colVariance=autoStats.variance()\n",
    "colStdDev=map(lambda x: math.sqrt(x), colVariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#place the means and std.dev values in a broadcast variable\n",
    "bcMeans=sc.broadcast(colMeans)\n",
    "bcStdDev=sc.broadcast(colStdDev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def centerAndScale(inVector) :\n",
    "    global bcMeans\n",
    "    global bcStdDev\n",
    "    \n",
    "    meanArray=bcMeans.value\n",
    "    stdArray=bcStdDev.value\n",
    "    \n",
    "    valueArray=inVector.toArray()\n",
    "    retArray=[]\n",
    "    for i in range(valueArray.size):\n",
    "        retArray.append( (valueArray[i] - meanArray[i]) /\\\n",
    "            stdArray[i] )\n",
    "    return Vectors.dense(retArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "csAuto = autoVector.map(centerAndScale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "csAuto.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create a Spark Data Frame\n",
    "autoRows=csAuto.map( lambda f:Row(features=f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "autoDf = sqlContext.createDataFrame(autoRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "autoDf.select(\"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans(k=3, seed=1)\n",
    "model = kmeans.fit(autoDf)\n",
    "predictions = model.transform(autoDf)\n",
    "predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Plot the results in a scatter plot\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def unstripData(instr) :\n",
    "    return ( instr[\"prediction\"], instr[\"features\"][0], \\\n",
    "        instr[\"features\"][1],instr[\"features\"][2],instr[\"features\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unstripped=predictions.map(unstripData)\n",
    "predList=unstripped.collect()\n",
    "predPd = pd.DataFrame(predList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "plt.cla()\n",
    "plt.scatter(predPd[3],predPd[4], c=predPd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
